{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c73f8-2094-469e-a6a3-28dc264a05bf",
   "metadata": {},
   "source": [
    "使用REINFORCE，模型训练了大概3000轮，基本可以稳赢，但是离理想中的21：0碾压局还差很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248b3c82-79c5-4a13-ad1f-279af2c18b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "Discrete(6)\n",
      "obs_dim 6400, act_dim 6\n",
      "开始从文件加载参数....\n",
      "从文件加载参数结束....\n",
      "Episode 0, Reward Sum 16.0.\n",
      "Episode 50, Reward Sum 13.0.\n",
      "Test reward: -3.6\n",
      "\n",
      "测试训练好的模型...\n",
      "63     -1.0 False\n",
      "126     -1.0 False\n",
      "174     1.0 False\n",
      "313     1.0 False\n",
      "378     -1.0 False\n",
      "506     1.0 False\n",
      "661     -1.0 False\n",
      "770     1.0 False\n",
      "909     1.0 False\n",
      "988     1.0 False\n",
      "1066     1.0 False\n",
      "1144     1.0 False\n",
      "1332     -1.0 False\n",
      "1395     -1.0 False\n",
      "1458     -1.0 False\n",
      "1521     -1.0 False\n",
      "1584     -1.0 False\n",
      "1647     -1.0 False\n",
      "1710     -1.0 False\n",
      "1773     -1.0 False\n",
      "1821     1.0 False\n",
      "1960     1.0 False\n",
      "2054     -1.0 False\n",
      "2102     1.0 False\n",
      "2235     1.0 False\n",
      "2360     -1.0 False\n",
      "2408     1.0 False\n",
      "2606     1.0 False\n",
      "2700     -1.0 False\n",
      "2748     1.0 False\n",
      "2826     1.0 False\n",
      "2965     1.0 False\n",
      "3044     1.0 False\n",
      "3122     1.0 False\n",
      "3262     1.0 False\n",
      "3340     1.0 True\n",
      "Counter({1.0: 21, -1.0: 15})\n",
      "你的得分为： 21 对手得分为： 15\n",
      "恭喜您赢了！！！\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, Counter\n",
    "import os\n",
    "from matplotlib import animation\n",
    "from PIL import Image\n",
    "import ale_py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "env = gym.make('ALE/Pong-v5')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "def preprocess(image):\n",
    "    #预处理 210x160x3 uint8 frame into 6400 (80x80) 1维 float vector\n",
    "    image = image[35:195]  # 裁剪\n",
    "    image = image[::2, ::2, 0]  # 下采样，缩放2倍\n",
    "    image[image == 144] = 0  # 擦除背景 (background type 1)\n",
    "    image[image == 109] = 0  # 擦除背景 \n",
    "    image[image != 0] = 1  # 转为灰度图，除了黑色外其他都是白色\n",
    "    return image.astype(np.float32).ravel()  # 打平,(6400,)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\" 使用全连接网络.\n",
    "    参数:\n",
    "        obs_dim (int): 观测空间的维度.\n",
    "        act_dim (int): 动作空间的维度.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(Model, self).__init__()\n",
    "        hid1_size = 256\n",
    "        hid2_size = 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(obs_dim, hid1_size)\n",
    "        self.fc2 = nn.Linear(hid1_size, hid2_size)\n",
    "        self.fc3 = nn.Linear(hid2_size, act_dim)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        h1 = F.relu(self.fc1(obs))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        prob = F.softmax(self.fc3(h2), dim=-1)\n",
    "        return prob\n",
    "\n",
    "# 梯度下降算法\n",
    "class PolicyGradient():\n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        prob = self.model(obs)\n",
    "        return prob\n",
    "    \n",
    "    def learn(self, obs, action, reward):\n",
    "        prob = self.model(obs)\n",
    "        dist = Categorical(prob)\n",
    "        log_prob = dist.log_prob(action.squeeze(-1))\n",
    "        loss = torch.mean(-1 * log_prob * reward.squeeze(-1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, algorithm):\n",
    "        self.alg = algorithm\n",
    "        \n",
    "        if os.path.exists(\"./savemodel\"):\n",
    "            print(\"开始从文件加载参数....\")\n",
    "            try:\n",
    "                self.load()\n",
    "                print(\"从文件加载参数结束....\")\n",
    "            except:\n",
    "                print(\"从文件加载参数失败，从0开始训练....\")\n",
    "    \n",
    "    def sample(self, obs):\n",
    "        \"\"\" 根据观测值 obs 采样（带探索）一个动作 \"\"\"\n",
    "        obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "        prob = self.alg.predict(obs)\n",
    "        prob = prob.detach().cpu().numpy()[0]\n",
    "        act = np.random.choice(len(prob), 1, p=prob)[0]  # 根据动作概率选取动作\n",
    "        return act\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        \"\"\" 根据观测值 obs 选择最优动作 \"\"\"\n",
    "        obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "        prob = self.alg.predict(obs)\n",
    "        act = prob.argmax().detach().cpu().item()  # 使用item()获取标量值\n",
    "        return act\n",
    "    \n",
    "    def learn(self, obs, act, reward):\n",
    "        \"\"\" 根据训练数据更新一次模型参数 \"\"\"\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        reward = np.expand_dims(reward, axis=-1)\n",
    "        \n",
    "        obs = torch.FloatTensor(obs).to(device)\n",
    "        act = torch.LongTensor(act).to(device)\n",
    "        reward = torch.FloatTensor(reward).to(device)\n",
    "        \n",
    "        loss = self.alg.learn(obs, act, reward)\n",
    "        return loss.detach().cpu().numpy()\n",
    "    \n",
    "    def save(self):\n",
    "        os.makedirs(\"./savemodel\", exist_ok=True)\n",
    "        torch.save(self.alg.model.state_dict(), './savemodel/PG-Pong_net.pdparams')\n",
    "        torch.save(self.alg.optimizer.state_dict(), \"./savemodel/opt.pdopt\")\n",
    "    \n",
    "    def load(self):\n",
    "        # 加载网络参数\n",
    "        model_state_dict = torch.load('./savemodel/PG-Pong_net.pdparams', map_location=device)\n",
    "        self.alg.model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # 加载优化器参数 \n",
    "        # optimizer_state_dict = torch.load(\"./savemodel/opt.pdopt\", map_location=device)\n",
    "        # self.alg.optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "# 训练一个episode\n",
    "def run_train_episode(agent, env):\n",
    "    obs_list, action_list, reward_list = [], [], []\n",
    "    obs, info = env.reset()\n",
    "    while True:\n",
    "        obs = preprocess(obs)  # from shape (210, 160, 3) to (6400,)\n",
    "        obs_list.append(obs)\n",
    "        action = agent.sample(obs)\n",
    "        \n",
    "        action_list.append(action)\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # if reward!=0:\n",
    "        #     print(\"reward: \",action)\n",
    "        \n",
    "        reward_list.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return obs_list, action_list, reward_list\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def run_evaluate_episodes(agent, env, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            obs = preprocess(obs)  # from shape (210, 160, 3) to (6400,)\n",
    "            action = agent.predict(obs)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            isOver = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if isOver:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)\n",
    "\n",
    "def calc_reward_to_go(reward_list, gamma=0.99):\n",
    "    \"\"\"calculate discounted reward\"\"\"\n",
    "    reward_arr = np.array(reward_list)\n",
    "    for i in range(len(reward_arr) - 2, -1, -1):\n",
    "        # G_t = r_t + γ·r_t+1 + ... = r_t + γ·G_t+1\n",
    "        reward_arr[i] += gamma * reward_arr[i + 1]\n",
    "    \n",
    "    # normalize episode rewards\n",
    "    reward_arr -= np.mean(reward_arr)\n",
    "    reward_arr /= np.std(reward_arr)\n",
    "    return reward_arr\n",
    "\n",
    "def main():\n",
    "    env = gym.make('ALE/Pong-v5')\n",
    "    obs_dim = 80 * 80\n",
    "    act_dim = env.action_space.n\n",
    "    print('obs_dim {}, act_dim {}'.format(obs_dim, act_dim))\n",
    "    \n",
    "    # 根据parl框架构建agent\n",
    "    LEARNING_RATE = 5e-4\n",
    "    model = Model(obs_dim=obs_dim, act_dim=act_dim).to(device)\n",
    "    alg = PolicyGradient(model, lr=LEARNING_RATE)\n",
    "    agent = Agent(alg)\n",
    "    \n",
    "    # twriter=LogWriter('./logs/PG_Pong')  # 注释掉visualdl\n",
    "    \n",
    "    for i in range(100):  # default 3000\n",
    "        obs_list, action_list, reward_list = run_train_episode(agent, env)\n",
    " \n",
    "        if i % 50 == 0:\n",
    "            print(\"Episode {}, Reward Sum {}.\".format(i, sum(reward_list)))\n",
    "        \n",
    "        batch_obs = np.array(obs_list)\n",
    "        batch_action = np.array(action_list)\n",
    "        batch_reward = calc_reward_to_go(reward_list)\n",
    "        \n",
    "\n",
    "        \n",
    "        agent.learn(batch_obs, batch_action, batch_reward)\n",
    "        last_test_total_reward = 0  \n",
    "        if (i + 1) % 100 == 0:\n",
    "            # render=True 查看显示效果\n",
    "            total_reward = run_evaluate_episodes(agent, env, render=False)\n",
    "            print('Test reward: {}'.format(total_reward))\n",
    "            \n",
    "            # save the parameters\n",
    "            if last_test_total_reward < total_reward:\n",
    "                last_test_total_reward = total_reward\n",
    "                agent.save()\n",
    "\n",
    "# 运行整个程序\n",
    "main()\n",
    "\n",
    "# # 9.使用训练好的网络进行测试并生成动图\n",
    "# def save_frames_as_gif(frames, filename):\n",
    "#     # Mess with this to change frame size\n",
    "#     plt.figure(figsize=(frames[0].shape[1]/100, frames[0].shape[0]/100), dpi=300)\n",
    "    \n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     def animate(i):\n",
    "#         patch.set_data(frames[i])\n",
    "    \n",
    "#     anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "#     anim.save(filename, writer='pillow', fps=60)\n",
    "\n",
    "# 从文件加载模型参数\n",
    "print(\"\\n测试训练好的模型...\")\n",
    "test_model = Model(6400, 6).to(device)\n",
    "if os.path.exists(\"./savemodel/PG-Pong_net.pdparams\"):\n",
    "    model_state_dict = torch.load(\"./savemodel/PG-Pong_net.pdparams\", map_location=device)\n",
    "    test_model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # 9.4 使用训练好的模型进行测试并保存过程为动图\n",
    "    # 创建带渲染模式的环境\n",
    "    env_test = gym.make('ALE/Pong-v5', render_mode='rgb_array')\n",
    "    \n",
    "    state, info = env_test.reset()\n",
    "    frames = []\n",
    "    done = False\n",
    "    i = 0\n",
    "    reward_list = []\n",
    "    \n",
    "    while not done:\n",
    "        # Gymnasium中正确的获取帧方式\n",
    "        frame = env_test.render()\n",
    "        if frame is not None:\n",
    "            frames.append(frame)\n",
    "        \n",
    "        obs = preprocess(state)\n",
    "        obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "        prob = test_model(obs)\n",
    "        action = prob.argmax().detach().cpu().item()  # 使用item()获取标量值\n",
    "        next_state, reward, terminated, truncated, _ = env_test.step(action)\n",
    "        done = terminated or truncated\n",
    "        if reward != 0:\n",
    "            reward_list.append(reward)\n",
    "            print(i, \"   \", reward, done)\n",
    "        state = next_state\n",
    "        i += 1\n",
    "    \n",
    "    reward_counter = Counter(reward_list)\n",
    "    print(reward_counter)\n",
    "    print(\"你的得分为：\", reward_counter.get(1.0, 0), '对手得分为：', reward_counter.get(-1.0, 0))\n",
    "    if reward_counter.get(1.0, 0) > reward_counter.get(-1.0, 0):\n",
    "        print(\"恭喜您赢了！！！\")\n",
    "    else:\n",
    "        print(\"惜败，惜败，训练一下智能体网络再来挑战吧QWQ\")\n",
    "    \n",
    "    # # 只有在收集到帧时才保存gif\n",
    "    # if frames:\n",
    "    #     print(f\"保存 {len(frames)} 帧为动图...\")\n",
    "    #     save_frames_as_gif(frames, filename=\"Pong-v5_trained.gif\")\n",
    "    #     print(\"动图已保存\")\n",
    "    # else:\n",
    "    #     print(\"未收集到渲染帧，跳过动图保存\")\n",
    "    \n",
    "    # env_test.close()\n",
    "else:\n",
    "    print(\"未找到训练好的模型文件，请先训练模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea262b33-aba1-4e95-af18-62db039429e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87702b0b-9de8-4adc-80be-401da9f117a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
