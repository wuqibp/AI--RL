{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ecaedf-1ee4-4392-bd51-76c9b19f6556",
   "metadata": {},
   "source": [
    "尝试使用DDPG解决pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d398ee69-9a8e-4135-bd9d-f237cafdb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        # 使用较小规模的网络\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)  # 添加LayerNorm提升训练稳定性\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.ln1(self.fc1(state)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        action = torch.tanh(self.fc3(x)) * self.max_action\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "class OUNoise:\n",
    "    #Ornstein-Uhlenbeck过程噪声，更适合惯性系统\n",
    "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(action_dim)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.action_dim = action_dim\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "    \n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state += dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state = np.array([e[0] for e in batch], dtype=np.float32)\n",
    "        action = np.array([e[1] for e in batch], dtype=np.float32)\n",
    "        reward = np.array([e[2] for e in batch], dtype=np.float32)\n",
    "        next_state = np.array([e[3] for e in batch], dtype=np.float32)\n",
    "        done = np.array([e[4] for e in batch], dtype=np.float32)\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        # 演员网络\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.actor_scheduler = StepLR(self.actor_optimizer, step_size=1000, gamma=0.95)  # 学习率调度\n",
    "        \n",
    "        # 评论家网络\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.critic_scheduler = StepLR(self.critic_optimizer, step_size=1000, gamma=0.95)\n",
    "        \n",
    "        # 经验回放\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        # OU噪声\n",
    "        self.ou_noise = OUNoise(action_dim)\n",
    "        \n",
    "        # 超参数\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.batch_size = 64\n",
    "        self.max_action = max_action\n",
    "        self.exploration_noise = 0.5  # 初始探索噪声\n",
    "        self.noise_decay = 0.9995    # 噪声衰减率\n",
    "        \n",
    "        # 训练计数器\n",
    "        self.train_step = 0\n",
    "    \n",
    "    def shaped_reward(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        reward reshaping\n",
    "        原始奖励: reward = -(θ² + 0.1*θ_dot² + 0.001*torque²)\n",
    "        \"\"\"\n",
    "       \n",
    "        cos_theta, sin_theta, theta_dot = state\n",
    "        theta = np.arctan2(sin_theta, cos_theta)\n",
    "        \n",
    "        # 强调直立状态\n",
    "        upright_bonus = 2.0 * (cos_theta + 1)  # 加倍奖励\n",
    "        \n",
    "        # 角度惩罚（更温和）\n",
    "        angle_penalty = -0.1 * (theta ** 2)\n",
    "        \n",
    "        # 角速度惩罚（鼓励更稳定）\n",
    "        angular_penalty = -0.05 * (theta_dot ** 2)\n",
    "        \n",
    "        # 扭矩惩罚（保持节能）\n",
    "        torque_penalty = -0.001 * (action[0] ** 2)\n",
    "        \n",
    "        # 组合奖励\n",
    "        new_reward = upright_bonus + angle_penalty + angular_penalty + torque_penalty\n",
    "        \n",
    "        return reward\n",
    "    def select_action(self, state, add_noise=True):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        \n",
    "        if add_noise:\n",
    "            # 使用OU噪声[8,9]\n",
    "            noise = self.exploration_noise * self.ou_noise.sample()\n",
    "            action = (action + noise).clip(-self.max_action, self.max_action)\n",
    "            # 衰减探索噪声\n",
    "            self.exploration_noise *= self.noise_decay\n",
    "            self.exploration_noise = max(self.exploration_noise, 0.1)  # 保持最小探索\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 采样批次\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Critic更新\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor_target(next_state)\n",
    "            target_Q = self.critic_target(next_state, next_action)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "        \n",
    "        current_Q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # 梯度裁剪防止爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor更新\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        # 更新学习率\n",
    "        self.actor_scheduler.step()\n",
    "        self.critic_scheduler.step()\n",
    "        \n",
    "        self.train_step += 1\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor.pth\")\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic.pth\")\n",
    "        print(f\"模型已保存: {filename}_actor.pth 和 {filename}_critic.pth\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n",
    "        print(f\"模型已加载: {filename}_actor.pth 和 {filename}_critic.pth\")\n",
    "\n",
    "def train_pendulum():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    print(f\"环境: Pendulum-v1\")\n",
    "    print(f\"状态维度: {state_dim}, 动作维度: {action_dim}, 最大动作: {max_action}\")\n",
    "    \n",
    "    agent = DDPG(state_dim, action_dim, max_action)\n",
    "    \n",
    "    num_episodes = 100\n",
    "    all_rewards = []\n",
    "    moving_avg_rewards = []\n",
    "    \n",
    "    # 训练循环\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        agent.ou_noise.reset()  # 每回合重置OU噪声\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = agent.select_action(state, add_noise=True)\n",
    "            next_state, raw_reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # 使用重塑后的奖励\n",
    "            shaped_reward = agent.shaped_reward(state, action, raw_reward, next_state)\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, shaped_reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += raw_reward  # 记录原始奖励用于评估\n",
    "            \n",
    "            # 延迟训练\n",
    "            if len(agent.replay_buffer) > 1000:\n",
    "                for _ in range(2):  # 每步训练2次\n",
    "                    agent.train()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        \n",
    "        # 计算移动平均\n",
    "        if episode < 10:\n",
    "            moving_avg = np.mean(all_rewards)\n",
    "        else:\n",
    "            moving_avg = np.mean(all_rewards[-10:])\n",
    "        moving_avg_rewards.append(moving_avg)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"回合 {episode:3d} | 奖励: {episode_reward:7.2f} | 平均奖励(10): {moving_avg:7.2f} | 噪声: {agent.exploration_noise:.3f}\")\n",
    "    \n",
    "    # # 绘制训练曲线，但是torch和matplotlib前后端冲突所以先注释掉\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(all_rewards, alpha=0.6, label='每回合奖励')\n",
    "    # plt.plot(moving_avg_rewards, 'r-', linewidth=2, label='移动平均 (10回合)')\n",
    "    # plt.xlabel('回合')\n",
    "    # plt.ylabel('奖励')\n",
    "    # plt.title('DDPG 在 Pendulum-v1 上的训练性能')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig('ddpg_training_performance.png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    \n",
    "    # 保存模型\n",
    "    agent.save(\"ddpg_pendulum\")\n",
    "    \n",
    "    # 测试训练好的策略\n",
    "    print(\"\\n测试策略...\")\n",
    "    test_rewards = []\n",
    "    for i in range(5):\n",
    "        state, info = env.reset()\n",
    "        test_reward = 0\n",
    "        for _ in range(200):\n",
    "            action = agent.select_action(state, add_noise=False)  # 测试时不加噪声\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            test_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        test_rewards.append(test_reward)\n",
    "        print(f\"测试 {i+1}: {test_reward:.2f}\")\n",
    "    \n",
    "    print(f\"\\n最终性能: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "    np.save('training_rewards.npy', np.array(all_rewards))\n",
    "    np.save('moving_avg_rewards.npy', np.array(moving_avg_rewards))\n",
    "    # 训练总结\n",
    "    print(\"\\n训练总结\")\n",
    "    print(f\"初始性能: {np.mean(all_rewards[:10]):.2f}\")\n",
    "    print(f\"最终性能: {np.mean(all_rewards[-10:]):.2f}\")\n",
    "    print(f\"最佳回合: {max(all_rewards):.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, all_rewards\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     agent, rewards = train_pendulum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07b223-78c1-482f-afcf-ffa963b8f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境: Pendulum-v1\n",
      "状态维度: 3, 动作维度: 1, 最大动作: 2.0\n",
      "回合   0 | 奖励: -1783.71 | 平均奖励(10): -1783.71 | 噪声: 0.452\n",
      "回合  10 | 奖励: -1170.29 | 平均奖励(10): -1300.39 | 噪声: 0.166\n",
      "回合  20 | 奖励: -1047.06 | 平均奖励(10): -775.33 | 噪声: 0.100\n",
      "回合  30 | 奖励: -125.25 | 平均奖励(10): -407.57 | 噪声: 0.100\n",
      "回合  40 | 奖励:   -3.70 | 平均奖励(10): -163.62 | 噪声: 0.100\n",
      "回合  50 | 奖励: -122.10 | 平均奖励(10): -142.52 | 噪声: 0.100\n",
      "回合  60 | 奖励: -121.53 | 平均奖励(10): -155.20 | 噪声: 0.100\n",
      "回合  70 | 奖励:   -7.40 | 平均奖励(10): -144.88 | 噪声: 0.100\n",
      "回合  80 | 奖励:  -13.98 | 平均奖励(10): -117.56 | 噪声: 0.100\n",
      "回合  90 | 奖励: -129.28 | 平均奖励(10): -148.91 | 噪声: 0.100\n",
      "模型已保存: ddpg_pendulum_actor.pth 和 ddpg_pendulum_critic.pth\n",
      "\n",
      "测试策略...\n",
      "测试 1: -137.32\n",
      "测试 2: -130.14\n",
      "测试 3: -132.34\n",
      "测试 4: -20.25\n",
      "测试 5: -134.21\n",
      "\n",
      "最终性能: -110.85 ± 45.36\n",
      "\n",
      "训练总结\n",
      "初始性能: -1361.73\n",
      "最终性能: -158.00\n",
      "最佳回合: -2.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_results_offline():\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        all_rewards = np.load('training_rewards.npy')\n",
    "        moving_avg_rewards = np.load('moving_avg_rewards.npy')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(all_rewards, alpha=0.6, label='每回合奖励')\n",
    "        plt.plot(moving_avg_rewards, 'r-', linewidth=2, label='移动平均 (10回合)')\n",
    "        plt.xlabel('回合')\n",
    "        plt.ylabel('奖励')\n",
    "        plt.title('DDPG 在 Pendulum-v1 上的训练性能')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('ddpg_training_performance.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"图表已保存为 ddpg_training_performance.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"绘图时出错: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent, rewards = train_pendulum()\n",
    "    # 训练完成后，在单独的进程中绘图\n",
    "    plot_results_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9167d7-2782-4c96-9de6-d8ee747b8163",
   "metadata": {},
   "source": [
    "最终的训练结果不尽如人意，应该需要进一步调参、改奖励函数  但是真的已经燃尽了，入门三月拼尽全力无法战胜..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cce08-2a07-4082-bd14-4d0b01e372b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
