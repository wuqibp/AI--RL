{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c390208-0e74-4321-a9f4-ab27a1a97690",
   "metadata": {},
   "source": [
    "读完这篇DPO论文后，我的理解：\n",
    "\n",
    "关于loss函数从Bradley-Terry模型的推导：BT模型在RLHF的场景下，它假设人类更偏好的回答y_w相对于y_l的概率可以表示为p(y_w > y_l) = sigmoid(r(x,y_w) - r(x,y_l))，所以传统RLHF需要先训练一个reward model去拟合这个r(x,y)，然后用RL去最大化这个reward。\n",
    "\n",
    "而DPO的关键突破就在于，**通过对公式3的RL优化目标求解，得到了最优策略π*的闭式解**：π*(y|x) ∝ π_ref(y|x) exp(r(x,y)/β)。把这个关系代入BT模型，就能直接用策略的概率比来表示偏好概率，从而绕过了显式的reward model。最终的DPO loss就是最大化偏好数据的似然，具体形式是-log σ(β log[π_θ(y_w|x)/π_ref(y_w|x)] - β log[π_θ(y_l|x)/π_ref(y_l|x)])，**这样就把优化reward转化成了直接优化概率分布**。\n",
    "\n",
    "**关于KL散度的作用**，我认为这是RLHF中一个非常重要的正则化项。如果没有KL约束，模型可能会过度优化reward而产生一些奇怪的输出，比如重复某些能获得高分的模式，或者完全偏离原始语言模型的分布。KL散度KL(π||π_ref)确保了微调后的模型不会偏离预训练模型太远，保持了语言的流畅性和多样性。这其实和PPO中的trust region思想类似，都是为了让优化过程更稳定，避免模型在某个方向上走得太远而崩溃。\n",
    "\n",
    "**对于RL在LLM上的潜力和应用**，查阅了一些资料，我觉得潜力还是很大的。除了已经很成功的使用DPO、RLHF用于对齐，RL还可以用于代码生成（通过执行结果作为reward）、对话系统（通过用户满意度反馈）、甚至是创造性写作（通过特定风格指标）。关于RL是否能学到人类偏好，基于这篇论文，我认为是可以的；但痛点在于——需要大量标注高质量的偏好数据。DPO本质上是在学习人类标注的偏好分布，标注质量好且覆盖面广，模型才能更好地捕捉到人类的价值观和偏好，缺数据、标数据这也是整个LLM领域的痛点。\n",
    "\n",
    "**至于RL能否完全替代预训练**，我觉得现阶段不太可能。预训练能让模型学会语言的基本结构和海量知识，而RL很难从零开始学习，需要大量的试错。我觉得RL更适合在已有的强大基础上做精细调整。从头用RL训练需要极其复杂的reward设计，而且样本效率会非常低。想象一下，要让模型仅通过reward信号学会语法、词汇、世界知识，这个探索空间太大太大了。\n",
    "\n",
    "从论文中的实验结果部分来看，一个有趣的发现是：DPO训练后的模型在分布外的任务上表现也有提升。论文中展示了在summarization任务上训练的模型，在dialogue等其他任务上也有改善，这说明通过偏好学习获得的能力有一定的泛化性，可能是因为模型学到了更通用的\"什么是好的回答\"的概念。但这种泛化是有限的，对于完全不同领域的输入，效果提升就没那么明显。\n",
    "\n",
    "总的来说，DPO提供了一种更简洁高效的方式来做偏好对齐，避免了传统RLHF的复杂性。但它还是依赖高质量的偏好数据和强大的预训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485c0f6-e971-49bf-8929-4d65283cd75b",
   "metadata": {},
   "source": [
    "[Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)\n",
    "这一篇文章是第一个RL预训练模型， 是RL从后训练角色迈向预训练的一大步，回答以下问题\n",
    "- 模型的训练数据/语料是什么格式？\n",
    "- 模型的奖励是如何定义？\n",
    "- 作者使用了哪些RL方法进行pretrain？了解这些方法并解释其在RLpretrain中的作用和必要性。\n",
    "\n",
    "\n",
    "\n",
    
    "第一个问题，关于训练数据的格式。

它用的不是那种为强化学习特意准备的、带标注的复杂数据。它用的其实就是普通的文本数据，和传统预训练一样，来自一个叫OmniMATH的数学数据集。它的聪明之处在于对数据做了“挑食”。它用Deepseek-R1的小模型先给文本里的每一个词位置打分，看预测下一个词的难度有多大。然后，在训练时，它会更多地让模型去学习和挑战那些最难预测的词。也就是说，数据本身还是纯文本，但训练过程是有侧重点的，专攻那些需要推理的硬骨头，而不是平均用力。

第二个问题，关于奖励是怎么定义的。

没有搞一套复杂的评分系统，比如判断句子流不流畅、事实准不准确等等。奖励规则就一条，即模型经过一番思考后，最终预测的下一个词，是不是和真实文本里的下一个词一模一样？ 如果一模一样，就给+1分；不一样，就是0分。这确保模型优化的唯一方向就是提高预测准确性。

第三个问题，关于用了什么RL方法以及为什么需要它。

论文里用的是策略梯度算法这一类的方法（具体实现基于他们自己的verl库和GRPO算法）。关键是，它不仅仅有一个生成答案的策略模型，还训练了一个价值模型。这个价值模型就像一个“先知”，它的任务是尝试判断当前这步推理走得好不好，预测最终拿到奖励的可能性有多大。

为什么用RL？ 因为传统的预训练方式是“老师教，学生听”，模型直接学习文本中的规律。而RPT想让模型模仿“主动思考”，RL框架在这里的作用就是为模型的“思考过程”提供反馈。价值模型会对生成的整段推理（思考轨迹）做出评价，虽然最终奖励只在答案处给出，但这个价值信号会渗透到思考过程的每一步，引导模型学会“什么样的思考方式更容易导向正确答案”。这样，预训练的目标就从“猜对下一个词”升华到了“学会如何推理出下一个词”，为模型打下了更强的推理能力基础，也让后续的精细调教能更快上手。
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09420f-bb78-4117-80e6-c3964a2b5703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
