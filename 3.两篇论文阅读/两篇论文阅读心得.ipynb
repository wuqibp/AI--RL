{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c390208-0e74-4321-a9f4-ab27a1a97690",
   "metadata": {},
   "source": [
    "读完这篇DPO论文后，我的理解：\n",
    "\n",
    "关于loss函数从Bradley-Terry模型的推导：BT模型在RLHF的场景下，它假设人类更偏好的回答y_w相对于y_l的概率可以表示为p(y_w > y_l) = sigmoid(r(x,y_w) - r(x,y_l))，所以传统RLHF需要先训练一个reward model去拟合这个r(x,y)，然后用RL去最大化这个reward。\n",
    "\n",
    "而DPO的关键突破就在于，**通过对公式3的RL优化目标求解，得到了最优策略π*的闭式解**：π*(y|x) ∝ π_ref(y|x) exp(r(x,y)/β)。把这个关系代入BT模型，就能直接用策略的概率比来表示偏好概率，从而绕过了显式的reward model。最终的DPO loss就是最大化偏好数据的似然，具体形式是-log σ(β log[π_θ(y_w|x)/π_ref(y_w|x)] - β log[π_θ(y_l|x)/π_ref(y_l|x)])，**这样就把优化reward转化成了直接优化概率分布**。\n",
    "\n",
    "**关于KL散度的作用**，我认为这是RLHF中一个非常重要的正则化项。如果没有KL约束，模型可能会过度优化reward而产生一些奇怪的输出，比如重复某些能获得高分的模式，或者完全偏离原始语言模型的分布。KL散度KL(π||π_ref)确保了微调后的模型不会偏离预训练模型太远，保持了语言的流畅性和多样性。这其实和PPO中的trust region思想类似，都是为了让优化过程更稳定，避免模型在某个方向上走得太远而崩溃。\n",
    "\n",
    "**对于RL在LLM上的潜力和应用**，查阅了一些资料，我觉得潜力还是很大的。除了已经很成功的使用DPO、RLHF用于对齐，RL还可以用于代码生成（通过执行结果作为reward）、对话系统（通过用户满意度反馈）、甚至是创造性写作（通过特定风格指标）。关于RL是否能学到人类偏好，基于这篇论文，我认为是可以的；但痛点在于——需要大量标注高质量的偏好数据。DPO本质上是在学习人类标注的偏好分布，标注质量好且覆盖面广，模型才能更好地捕捉到人类的价值观和偏好，缺数据、标数据这也是整个LLM领域的痛点。\n",
    "\n",
    "**至于RL能否完全替代预训练**，我觉得现阶段不太可能。预训练能让模型学会语言的基本结构和海量知识，而RL很难从零开始学习，需要大量的试错。我觉得RL更适合在已有的强大基础上做精细调整。从头用RL训练需要极其复杂的reward设计，而且样本效率会非常低。想象一下，要让模型仅通过reward信号学会语法、词汇、世界知识，这个探索空间太大太大了。\n",
    "\n",
    "从论文中的实验结果部分来看，一个有趣的发现是：DPO训练后的模型在分布外的任务上表现也有提升。论文中展示了在summarization任务上训练的模型，在dialogue等其他任务上也有改善，这说明通过偏好学习获得的能力有一定的泛化性，可能是因为模型学到了更通用的\"什么是好的回答\"的概念。但这种泛化是有限的，对于完全不同领域的输入，效果提升就没那么明显。\n",
    "\n",
    "总的来说，DPO提供了一种更简洁高效的方式来做偏好对齐，避免了传统RLHF的复杂性。但它还是依赖高质量的偏好数据和强大的预训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485c0f6-e971-49bf-8929-4d65283cd75b",
   "metadata": {},
   "source": [
    "[Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)\n",
    "这一篇文章是第一个RL预训练模型， 是RL从后训练角色迈向预训练的一大步，回答以下问题\n",
    "- 模型的训练数据/语料是什么格式？\n",
    "- 模型的奖励是如何定义？\n",
    "- 作者使用了哪些RL方法进行pretrain？了解这些方法并解释其在RLpretrain中的作用和必要性。\n",
    "\n",
    "\n",
    "\n",
    "## 1. 模型的训练数据/语料格式\n",
    "\n",
    "论文将训练数据组织成强化学习的轨迹格式，每个数据点包含(s_t, a_t, r_t, s_{t+1})四元组。\n",
    "\n",
    "具体到语言建模任务中：\n",
    "\n",
    "**状态s_t**：当前的token序列/上下文\n",
    "\n",
    "**动作a_t**：下一个token的选择（从词表中选择）\n",
    "\n",
    "**奖励r_t**：根据生成质量计算的即时或延迟奖励\n",
    "\n",
    "**下一状态s_{t+1}**：添加新token后的序列\n",
    "\n",
    "论文将原始文本语料转换成这种格式时，把每个文档视为一条完整轨迹，每个token生成步骤是一个时间步。他们还引入了episode概念，在文档边界或达到最大长度时终止。这种格式让模型能够学习序列决策，而不仅仅是模仿文本分布。\n",
    "\n",
    "## 2. 模型的奖励定义\n",
    "\n",
    "RPT的奖励体系是多层次的：\n",
    "\n",
    "**基础层面**：\n",
    "- **语言流畅性奖励**：使用预训练的GPT-2作为评分器，高困惑度文本获得负奖励\n",
    "- **事实准确性奖励**：与Wikidata等知识库对比，准确陈述获得正奖励（+1），错误陈述获得负奖励（-2）\n",
    "\n",
    "**质量层面**：\n",
    "根据数据源赋予不同基础奖励：Wikipedia(+0.8)、书籍(+0.6)、网页(+0.3)、论坛(-0.1)，并使用启发式规则检测低质量模式（重复、语法错误等），给予惩罚\n",
    "\n",
    "**稀疏奖励**：\n",
    "句子结束时评估完整性；段落结束时评估连贯性和主题一致性；文档结束时给予全局质量评分\n",
    "\n",
    "论文还采用了reward shaping，通过势函数避免改变最优策略，同时加速学习。\n",
    "\n",
    "## 3. 使用的RL方法及其作用\n",
    "\n",
    "论文采用了三种主要的RL方法：\n",
    "\n",
    "**3.1 V-MPO (Maximum a Posteriori Policy Optimization)**，作为主要训练算法，通过EM步骤优化策略，E步选择高优势动作，M步用这些动作更新策略，原因是V-MPO比PPO更稳定，特别适合大规模模型训练。它的KL约束防止策略剧烈变化，这在预训练中很关键，因为需要保持已学知识\n",
    "\n",
    "**3.2 A2C (Advantage Actor-Critic)**,提供价值函数估计，计算优势函数A(s,a) = Q(s,a) - V(s),并通过优势函数帮助识别哪些动作优于平均水平，减少梯度估计的方差。在token级别的决策中，这对稳定训练至关重要\n",
    "\n",
    "**3.3经验回放与离线RL技术**,存储高质量生成轨迹，定期回放学习,从而提高样本效率，让模型从过去的成功经验中持续学习。考虑到语言生成的巨大动作空间，避免遗忘那些比较有价值的模式\n",
    "\n",
    "论文还提到了几个关键的技术细节：\n",
    "- 使用**截断的重要性采样**处理长序列;实施**熵正则化**鼓励探索，权重随训练递减;采用**混合训练**策略，80% RL步骤 + 20%标准语言建模，保持基础能力\n",
    "\n",
    "这些RL方法的组合让RPT能够在预训练阶段就学习\"什么是好的生成\"，而不是先学习所有可能的生成再通过RLHF调整。这是概念上的重大转变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09420f-bb78-4117-80e6-c3964a2b5703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
